{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5WwZkKYt89+OaZ96fG60h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonychackotc/nlp-framework/blob/main/nlp_framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzRG97mpUFeE",
        "outputId": "1e1e48cd-4b08-4379-c047-fed28cab270a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app1.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app1.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "!pip install -q pyngrok\n",
        "!pip install -q localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aRtIR-LUHn3",
        "outputId": "5b44440a-b09d-4e5e-a40b-408abca91684"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement localtunnel (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for localtunnel\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit pandas numpy nltk wordcloud matplotlib scikit-learn seaborn spacy textblob langdetect googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXR304ILqMBt",
        "outputId": "22cbf821-56ae-4eda-e6c8-3875ea40447a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.42.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.11/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.11/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.11/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.11/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.25.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed nltk-3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creat nlp application using streamlit\n",
        "\n",
        "# upload a dataset -( csv, excell,text,json)\n",
        "# analysis a dataset and tell about dataset using word cloud\n",
        "\n",
        "# create a sepearate tab for each analysis\n",
        "\n",
        "# 1) Text Preprocessing - seperate tabs\n",
        "# - sub tabs\n",
        "# a)Tokenization shows ( sentense tokenisation, word tokenisation, letter tokenisation)\n",
        "# b)stop words removal\n",
        "# c)lowercase\n",
        "# d) remove punchuation and special characters\n",
        "# e) lowercasing\n",
        "# f) lemattization\n",
        "\n",
        "# 2) Feature Extraction (Vectorisation) - seperate tabs\n",
        "# -sub tabs\n",
        "# a) Bag of words\n",
        "# b)TF-IDF\n",
        "# TF - Term Frequency ( words repeated)\n",
        "# IDF - Inverse Document Frequency ( Log Value Check which Value was less importance that value was removed)\n",
        "# c)word embedding (Assuming Propabalities values - relationship)\n",
        "# d) n- grams (unigrams,bigrams,trigrams)\n",
        "# e) parts of speech\n",
        "\n",
        "# 3) model train - seperate tabs\n",
        "# choose algorithm\n",
        "# - Navie bayes classifier\n",
        "# - svm\n",
        "# - decession tree\n",
        "# - random forest\n",
        "\n",
        "# show multi dimensional chart pca\n",
        "\n",
        "# model evaluation\n",
        "# -show loading time\n",
        "\n",
        "# show both train and test\n",
        "# -accuracy\n",
        "# -precession\n",
        "# -recall\n",
        "# -f1 score\n",
        "# show confusion matrix chart and table\n",
        "\n",
        "# download train and test data\n",
        "\n",
        "# 4) Future Prediction - seperate tabs\n",
        "\n",
        "# detect this data based on\n",
        "\n",
        "# if ham or spam based means show type the content or email show spam or ham\n",
        "# else content show sentiment analysis\n",
        "# else show topics prediction based on summary\n",
        "# else - language detection\n",
        "# else - Document classification\n",
        "\n",
        "# show all in one\n",
        "\n"
      ],
      "metadata": {
        "id": "3TNLWstMUpeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Uninstall NLTK and delete its data entirely\n",
        "!pip uninstall -y nltk\n",
        "!rm -rf /root/nltk_data\n",
        "!rm -rf /usr/local/nltk_data\n",
        "\n",
        "# Step 2: Reinstall a stable version of NLTK\n",
        "!pip install nltk==3.8.1  # Pin to a stable version\n",
        "\n",
        "# Step 3: Re-download necessary datasets\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Step 4: Print data path to verify\n",
        "print(nltk.data.path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aVxCKieTEXE",
        "outputId": "699a4c44-aff1-4f3f-a903-36b28709549d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nltk 3.9.1\n",
            "Uninstalling nltk-3.9.1:\n",
            "  Successfully uninstalled nltk-3.9.1\n",
            "Collecting nltk==3.8.1\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (4.67.1)\n",
            "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Installing collected packages: nltk\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nltk-3.8.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app1.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "from langdetect import detect\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function to load data\n",
        "def load_data(uploaded_file):\n",
        "    if uploaded_file.name.endswith('.csv'):\n",
        "        data = pd.read_csv(uploaded_file)\n",
        "    elif uploaded_file.name.endswith('.xlsx'):\n",
        "        data = pd.read_excel(uploaded_file)\n",
        "    elif uploaded_file.name.endswith('.json'):\n",
        "        data = pd.read_json(uploaded_file)\n",
        "    else:\n",
        "        data = pd.read_csv(uploaded_file, sep='\\t')\n",
        "    return data\n",
        "\n",
        "# Function to generate word cloud\n",
        "def generate_wordcloud(text):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    st.pyplot(plt)\n",
        "\n",
        "# Load spaCy model only when needed\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def load_spacy_model():\n",
        "    import spacy\n",
        "    return spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load GPT-2 model and tokenizer only when needed\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def load_gpt2_model():\n",
        "    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "    model_name = \"gpt2\"\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "# Load translator only when needed\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def load_translator():\n",
        "    from googletrans import Translator\n",
        "    return Translator()\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"NLP Application\")\n",
        "\n",
        "    # Initialize session state\n",
        "    if 'model' not in st.session_state:\n",
        "        st.session_state.model = None\n",
        "    if 'tfidf_vectorizer' not in st.session_state:\n",
        "        st.session_state.tfidf_vectorizer = None\n",
        "    if 'X_tfidf' not in st.session_state:\n",
        "        st.session_state.X_tfidf = None\n",
        "\n",
        "    # Upload dataset\n",
        "    uploaded_file = st.file_uploader(\"Upload your dataset (CSV, Excel, JSON, Text)\", type=[\"csv\", \"xlsx\", \"json\", \"txt\"])\n",
        "    if uploaded_file is not None:\n",
        "        data = load_data(uploaded_file)\n",
        "        st.write(\"Dataset Overview:\")\n",
        "        st.write(data.head())\n",
        "\n",
        "        # Select feature and target columns\n",
        "        st.subheader(\"Select Feature and Target Columns\")\n",
        "        feature_column = st.multiselect(\"Select Feature Column\", data.columns)\n",
        "        target_column = st.selectbox(\"Select Target Column\", data.columns)\n",
        "\n",
        "        # Word Cloud\n",
        "        st.subheader(\"Word Cloud\")\n",
        "        if feature_column:\n",
        "            combined_text = ' '.join(data[feature_column].apply(lambda x: ' '.join(x.astype(str)), axis=1))\n",
        "            generate_wordcloud(combined_text)\n",
        "        else:\n",
        "            st.warning(\"Please select at least one feature column to generate the word cloud.\")\n",
        "\n",
        "        # Tabs for different analyses\n",
        "        tab1, tab2, tab3, tab4 = st.tabs([\"Text Preprocessing\", \"Feature Extraction\", \"Model Training\", \"Future Prediction\"])\n",
        "\n",
        "        with tab1:\n",
        "            st.header(\"Text Preprocessing\")\n",
        "            sub_tab1, sub_tab2, sub_tab3, sub_tab4, sub_tab5, sub_tab6 = st.tabs([\"Tokenization\", \"Stop Words Removal\", \"Lowercasing\", \"Remove Punctuation\", \"Lemmatization\", \"Lowercasing\"])\n",
        "\n",
        "            with sub_tab1:\n",
        "                st.subheader(\"Tokenization\")\n",
        "                st.write(\"Sentence Tokenization:\")\n",
        "                sentences = sent_tokenize(combined_text)\n",
        "                st.write(sentences)\n",
        "\n",
        "                st.write(\"Word Tokenization:\")\n",
        "                words = word_tokenize(combined_text)\n",
        "                st.write(words)\n",
        "\n",
        "                st.write(\"Letter Tokenization:\")\n",
        "                letters = list(combined_text)\n",
        "                st.write(letters)\n",
        "\n",
        "            with sub_tab2:\n",
        "                st.subheader(\"Stop Words Removal\")\n",
        "                stop_words = set(stopwords.words('english'))\n",
        "                filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "                st.write(filtered_words)\n",
        "\n",
        "            with sub_tab3:\n",
        "                st.subheader(\"Lowercasing\")\n",
        "                lowercased_words = [word.lower() for word in words]\n",
        "                st.write(lowercased_words)\n",
        "\n",
        "            with sub_tab4:\n",
        "                st.subheader(\"Remove Punctuation and Special Characters\")\n",
        "                import string\n",
        "                cleaned_words = [word for word in words if word.isalnum()]\n",
        "                st.write(cleaned_words)\n",
        "\n",
        "            with sub_tab5:\n",
        "                st.subheader(\"Lemmatization\")\n",
        "                lemmatizer = WordNetLemmatizer()\n",
        "                lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "                st.write(lemmatized_words)\n",
        "\n",
        "            with sub_tab6:\n",
        "                st.subheader(\"Lowercasing\")\n",
        "                lowercased_words = [word.lower() for word in words]\n",
        "                st.write(lowercased_words)\n",
        "\n",
        "        with tab2:\n",
        "            st.header(\"Feature Extraction\")\n",
        "            sub_tab1, sub_tab2, sub_tab3, sub_tab4, sub_tab5 = st.tabs([\"Bag of Words\", \"TF-IDF\", \"Word Embedding\", \"N-grams\", \"Parts of Speech\"])\n",
        "\n",
        "            with sub_tab1:\n",
        "                st.subheader(\"Bag of Words\")\n",
        "                vectorizer = CountVectorizer()\n",
        "                X = vectorizer.fit_transform(data[feature_column])\n",
        "                st.write(X.toarray())\n",
        "\n",
        "            with sub_tab2:\n",
        "                st.subheader(\"TF-IDF\")\n",
        "                if feature_column:\n",
        "                    combined_text_series = data[feature_column].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
        "                    tfidf_vectorizer = TfidfVectorizer()\n",
        "                    X_tfidf = tfidf_vectorizer.fit_transform(combined_text_series)\n",
        "                    st.write(X_tfidf.toarray())\n",
        "                    st.session_state.tfidf_vectorizer = tfidf_vectorizer\n",
        "                    st.session_state.X_tfidf = X_tfidf\n",
        "                else:\n",
        "                    st.warning(\"Please select at least one feature column.\")\n",
        "\n",
        "            with sub_tab3:\n",
        "                st.subheader(\"Word Embedding\")\n",
        "                nlp = load_spacy_model()\n",
        "                doc = nlp(combined_text)\n",
        "                st.write([token.vector for token in doc])\n",
        "\n",
        "            with sub_tab4:\n",
        "                st.subheader(\"N-grams\")\n",
        "                st.write(\"Unigrams:\")\n",
        "                unigrams = list(nltk.ngrams(words, 1))\n",
        "                st.write(unigrams)\n",
        "\n",
        "                st.write(\"Bigrams:\")\n",
        "                bigrams = list(nltk.ngrams(words, 2))\n",
        "                st.write(bigrams)\n",
        "\n",
        "                st.write(\"Trigrams:\")\n",
        "                trigrams = list(nltk.ngrams(words, 3))\n",
        "                st.write(trigrams)\n",
        "\n",
        "            with sub_tab5:\n",
        "                st.subheader(\"Parts of Speech\")\n",
        "                nlp = load_spacy_model()\n",
        "                doc = nlp(combined_text)\n",
        "                st.write([(token.text, token.pos_) for token in doc])\n",
        "\n",
        "        with tab3:\n",
        "            st.header(\"Model Training\")\n",
        "            st.subheader(\"Choose Algorithm\")\n",
        "            algorithm = st.selectbox(\"Select Algorithm\", [\"Naive Bayes\", \"SVM\", \"Decision Tree\", \"Random Forest\", \"Gradient Boosting\"])\n",
        "\n",
        "            # Cross-validation options\n",
        "            st.subheader(\"Cross-Validation\")\n",
        "            cv_method = st.selectbox(\"Select Cross-Validation Method\", [\"None\", \"K-Fold\", \"Stratified K-Fold\", \"Hold Out\"])\n",
        "            if cv_method != \"None\":\n",
        "                cv_folds = st.number_input(\"Number of Folds\", min_value=2, max_value=10, value=5)\n",
        "\n",
        "            # Hyperparameter tuning options\n",
        "            st.subheader(\"Hyperparameter Tuning\")\n",
        "            tuning_method = st.selectbox(\"Select Tuning Method\", [\"None\", \"Grid Search\", \"Random Search\"])\n",
        "\n",
        "            if st.button(\"Train Model\"):\n",
        "                y = data[target_column]\n",
        "                if st.session_state.X_tfidf.shape[0] != data[target_column].shape[0]:\n",
        "                    st.error(f\"Number of samples in X_tfidf ({st.session_state.X_tfidf.shape[0]}) and y ({data[target_column].shape[0]}) do not match. Please check your data.\")\n",
        "                else:\n",
        "                    X_train, X_test, y_train, y_test = train_test_split(st.session_state.X_tfidf, data[target_column], test_size=0.2, random_state=42)\n",
        "\n",
        "                if algorithm == \"Naive Bayes\":\n",
        "                    model = MultinomialNB()\n",
        "                elif algorithm == \"SVM\":\n",
        "                    model = SVC()\n",
        "                elif algorithm == \"Decision Tree\":\n",
        "                    model = DecisionTreeClassifier()\n",
        "                elif algorithm == \"Random Forest\":\n",
        "                    model = RandomForestClassifier()\n",
        "                elif algorithm == \"Gradient Boosting\":\n",
        "                    model = GradientBoostingClassifier()\n",
        "\n",
        "                # Hyperparameter tuning\n",
        "                if tuning_method == \"Grid Search\":\n",
        "                    if algorithm == \"Random Forest\":\n",
        "                        param_grid = {\n",
        "                            'n_estimators': [100, 200, 300],\n",
        "                            'max_depth': [None, 10, 20],\n",
        "                            'min_samples_split': [2, 5, 10]\n",
        "                        }\n",
        "                        grid_search = GridSearchCV(model, param_grid, cv=cv_folds if cv_method != \"None\" else 5)\n",
        "                        grid_search.fit(X_train, y_train)\n",
        "                        model = grid_search.best_estimator_\n",
        "                        st.write(\"Best Parameters:\", grid_search.best_params_)\n",
        "                elif tuning_method == \"Random Search\":\n",
        "                    if algorithm == \"Random Forest\":\n",
        "                        param_dist = {\n",
        "                            'n_estimators': [100, 200, 300],\n",
        "                            'max_depth': [None, 10, 20],\n",
        "                            'min_samples_split': [2, 5, 10]\n",
        "                        }\n",
        "                        random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=cv_folds if cv_method != \"None\" else 5)\n",
        "                        random_search.fit(X_train, y_train)\n",
        "                        model = random_search.best_estimator_\n",
        "                        st.write(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "                # Cross-validation\n",
        "                if cv_method == \"K-Fold\":\n",
        "                    cv_scores = cross_val_score(model, X_train, y_train, cv=cv_folds)\n",
        "                    st.write(f\"Cross-Validation Accuracy: {cv_scores.mean():.2f} (±{cv_scores.std():.2f})\")\n",
        "                elif cv_method == \"Stratified K-Fold\":\n",
        "                    skf = StratifiedKFold(n_splits=cv_folds)\n",
        "                    cv_scores = cross_val_score(model, X_train, y_train, cv=skf)\n",
        "                    st.write(f\"Stratified K-Fold Accuracy: {cv_scores.mean():.2f} (±{cv_scores.std():.2f})\")\n",
        "\n",
        "                # Train the model\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "\n",
        "                st.subheader(\"Model Evaluation\")\n",
        "                st.write(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "                st.write(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "                st.write(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
        "                st.write(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "                st.subheader(\"Confusion Matrix\")\n",
        "                cm = confusion_matrix(y_test, y_pred)\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "                plt.xlabel('Predicted')\n",
        "                plt.ylabel('Actual')\n",
        "                plt.title('Confusion Matrix')\n",
        "                st.pyplot(plt)\n",
        "\n",
        "                st.subheader(\"Confusion Matrix Explanation\")\n",
        "                st.write(\"\"\"\n",
        "                - **True Positive (TP)**: The model correctly predicted the positive class.\n",
        "                - **False Positive (FP)**: The model incorrectly predicted the positive class (Type I Error).\n",
        "                - **True Negative (TN)**: The model correctly predicted the negative class.\n",
        "                - **False Negative (FN)**: The model incorrectly predicted the negative class (Type II Error).\n",
        "                \"\"\")\n",
        "                st.write(f\"\"\"\n",
        "                - **True Positives (TP)**: {cm[1, 1]}\n",
        "                - **False Positives (FP)**: {cm[0, 1]}\n",
        "                - **True Negatives (TN)**: {cm[0, 0]}\n",
        "                - **False Negatives (FN)**: {cm[1, 0]}\n",
        "                \"\"\")\n",
        "\n",
        "                # Store the trained model in session state\n",
        "                st.session_state.model = model\n",
        "\n",
        "        with tab4:\n",
        "            st.header(\"Future Prediction\")\n",
        "            st.subheader(\"Select Prediction Type\")\n",
        "            prediction_type = st.selectbox(\"Select Prediction Type\", [\n",
        "                \"Spam/Ham Detection\",\n",
        "                \"Sentiment Analysis\",\n",
        "                \"Topic Prediction\",\n",
        "                \"Document Classification\",\n",
        "                \"Language Detection\",\n",
        "                \"Translate to Tamil\",\n",
        "                \"Text Autogeneration\"\n",
        "            ])\n",
        "\n",
        "            input_text = st.text_area(\"Input Text\")\n",
        "\n",
        "            if st.button(\"Predict\"):\n",
        "                if st.session_state.model is None:\n",
        "                    st.warning(\"Please train a model in the 'Model Training' tab first.\")\n",
        "                else:\n",
        "                    if prediction_type == \"Spam/Ham Detection\":\n",
        "                        input_vector = st.session_state.tfidf_vectorizer.transform([input_text])\n",
        "                        prediction = st.session_state.model.predict(input_vector)\n",
        "                        st.write(\"Prediction:\", \"Spam\" if prediction[0] == 1 else \"Ham\")\n",
        "\n",
        "                    elif prediction_type == \"Sentiment Analysis\":\n",
        "                        blob = TextBlob(input_text)\n",
        "                        sentiment = blob.sentiment\n",
        "                        st.write(\"Sentiment Polarity:\", sentiment.polarity)\n",
        "                        st.write(\"Sentiment Subjectivity:\", sentiment.subjectivity)\n",
        "                        st.write(\"Sentiment:\", \"Positive\" if sentiment.polarity > 0 else \"Negative\" if sentiment.polarity < 0 else \"Neutral\")\n",
        "\n",
        "                    elif prediction_type == \"Topic Prediction\":\n",
        "                        lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "                        lda.fit(st.session_state.X_tfidf)\n",
        "                        input_vector = st.session_state.tfidf_vectorizer.transform([input_text])\n",
        "                        topic_distribution = lda.transform(input_vector)\n",
        "                        st.write(\"Topic Distribution:\", topic_distribution)\n",
        "                        st.write(\"Predicted Topic:\", np.argmax(topic_distribution))\n",
        "\n",
        "                    elif prediction_type == \"Document Classification\":\n",
        "                        input_vector = st.session_state.tfidf_vectorizer.transform([input_text])\n",
        "                        prediction = st.session_state.model.predict(input_vector)\n",
        "                        st.write(\"Predicted Class:\", prediction[0])\n",
        "\n",
        "                    elif prediction_type == \"Language Detection\":\n",
        "                        language = detect(input_text)\n",
        "                        st.write(\"Detected Language:\", language)\n",
        "\n",
        "                    elif prediction_type == \"Translate to Tamil\":\n",
        "                        translator = load_translator()\n",
        "                        translation = translator.translate(input_text, dest='ta')\n",
        "                        st.write(\"Translated Text:\", translation.text)\n",
        "\n",
        "                    elif prediction_type == \"Text Autogeneration\":\n",
        "                        tokenizer, model = load_gpt2_model()\n",
        "                        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "                        outputs = model.generate(inputs, max_length=200, num_return_sequences=1, temperature=0.7)\n",
        "                        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                        st.write(\"Generated Text:\", generated_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGRT_ZWWvBev",
        "outputId": "bacb6395-95ed-4c36-acc0-226580c50dba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace 'YOUR_AUTHTOKEN' with your actual ngrok authtoken\n",
        "ngrok.set_auth_token(\"2rI2XurhgC2fxlYDtteHntWpCJf_5b1kDx2SLmwgq8GukDEyc\")\n",
        "\n",
        "# Run the Streamlit app in the background\n",
        "!streamlit run app1.py &>/dev/null&\n",
        "\n",
        "# Create a public URL using ngrok\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"Streamlit app is running at {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Trying to run with localtunnel\")\n",
        "    !streamlit run app1.py &>/content/logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctapgATjZyWa",
        "outputId": "dfc97224-6e24-433e-c67d-0ff7a5f006d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is running at NgrokTunnel: \"https://816a-34-80-204-3.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SF1m1Atb9K7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}